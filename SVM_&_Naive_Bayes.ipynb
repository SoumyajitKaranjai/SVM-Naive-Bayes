{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is a Support Vector Machine (SVM), and how does it work?\n",
        "\n",
        "**Support Vector Machine (SVM):-**\n",
        "A Support Vector Machine (SVM) is a supervised machine learning algorithm primarily used for classification and regression tasks. It works by finding an optimal hyperplane that best separates data points of different classes in a feature space. The goal of the SVM is to maximize the margin, which is the distance between the hyperplane and the closest data points from each class. These closest points are called support vectors, and they are crucial in defining the position and orientation of the hyperplane.\n",
        "\n",
        "How SVM Works:\n",
        "\n",
        "a. Hyperplane: This is the decision boundary that separates different classes. In 2D space, it is a line; in higher dimensions, it becomes a plane or hyperplane.\n",
        "\n",
        "b. Support Vectors: These are the data points nearest to the hyperplane and they influence the position and angle of the hyperplane. Only these points are used for the learning process.\n",
        "\n",
        "c. Margin Maximization: SVM chooses the hyperplane that maximizes the margin, providing the largest separation between classes, which generally improves the model’s generalization on new data.\n",
        "\n",
        "d. Linearly Separable Data: When data can be separated by a straight line or flat hyperplane, SVM finds the best linear boundary.\n",
        "\n",
        "e. Non-linear Data and Kernel Trick: If data is not linearly separable, SVM uses kernel functions (e.g., polynomial, radial basis function (RBF), sigmoid) to map data into a higher-dimensional space where a linear separator can be found. This approach is called the \"kernel trick.\"\n",
        "\n",
        "f. Soft Margin: To handle noisy data and allow some misclassifications, SVM uses a soft margin, balancing margin maximization and classification errors via a regularization parameter.\n",
        "\n",
        "Question 2: Explain the difference between Hard Margin and Soft Margin SVM.\n",
        "\n",
        "**Hard Margin SVM:-**\n",
        "\n",
        "a. Aims to find a hyperplane that strictly separates the classes with no misclassification allowed.\n",
        "\n",
        "b. Maximizes the margin between the two classes, with data points lying exactly on the margin boundary or outside it.\n",
        "\n",
        "c. Requires data to be perfectly linearly separable without any noise or outliers.\n",
        "\n",
        "d. Is highly sensitive to outliers; even a single point inside the margin or misclassified can disrupt the model and make it fail.\n",
        "\n",
        "e. Does not involve a regularization parameter for balancing margin and errors since misclassification is not permitted.\n",
        "\n",
        "f. Optimization objective: minimize the norm of the weight vector subject to every point being correctly classified with a margin of at least 1.\n",
        "\n",
        "**Soft Margin SVM:-**\n",
        "\n",
        "a. Allows some misclassifications or margin violations by introducing slack variables.\n",
        "\n",
        "b. Balances between maximizing margin and minimizing classification errors using a regularization parameter C.\n",
        "\n",
        "c. Suitable for non-linearly separable or noisy datasets where perfect separation is not possible.\n",
        "\n",
        "d. The parameter C controls the trade-off:\n",
        "\n",
        "A high C emphasizes fewer misclassifications (closer to hard margin).\n",
        "\n",
        "A low C allows more errors but leads to a wider margin and potentially better generalization.\n",
        "\n",
        "e. More flexible and robust for real-world data.\n",
        "\n",
        "f. Optimization involves minimizing a combination of the margin (weight norm) and the sum of slack variables weighted by C.\n",
        "\n",
        "Question 3: What is the Kernel Trick in SVM? Give one example of a kernel and explain its use case.\n",
        "\n",
        "**Kernel Trick in SVM:-**\n",
        "The Kernel Trick in Support Vector Machines (SVM) is a technique that allows SVMs to efficiently perform classification on non-linearly separable data by implicitly mapping the input data into a higher-dimensional feature space. Instead of explicitly transforming the data into this higher space (which could be computationally expensive or infeasible), the kernel trick uses a kernel function to compute the inner products of data points in this high-dimensional space directly, enabling the SVM to find a linear separating hyperplane there.\n",
        "\n",
        "How the Kernel Trick Works:\n",
        "\n",
        "a. It replaces the dot product in the input feature space with a kernel function that corresponds to the dot product in a transformed, higher-dimensional space.\n",
        "\n",
        "b. This implicit mapping helps convert a non-linear classification problem into a linear one in the new space.\n",
        "\n",
        "c. The kernel function effectively measures similarity between pairs of data points in this higher-dimensional space without performing the expensive transformation explicitly.\n",
        "\n",
        "Example Kernel: Radial Basis Function (RBF) Kernel\n",
        "\n",
        "a. The RBF kernel, also known as the Gaussian kernel, is defined as:\n",
        "\n",
        "K(x,y)=exp⁡(−(∥x−y∥)2/2(σ)2)\n",
        "\n",
        "b. It measures similarity based on the distance between two points x and y and can handle very complex, localized decision boundaries.\n",
        "\n",
        "c. Use case: The RBF kernel is widely used for datasets where the relationship between class labels and features is non-linear and complicated. It works well in scenarios like image classification, bioinformatics, and speech recognition because it can create flexible, smooth boundaries that separate classes in a non-linear manner.\n",
        "\n",
        "Question 4: What is a Naive Bayes Classifier, and why is it called “naive”?\n",
        "\n",
        "**Naive Bayes Classifier:-**\n",
        "A Naive Bayes Classifier is a supervised machine learning classification algorithm based on Bayes' Theorem. It predicts the class of a data point by calculating the probabilities of each class given the features of the data and selects the class with the highest posterior probability.\n",
        "\n",
        "It is called “naive” because:\n",
        "\n",
        "a. The independence assumption is \"naïve\" because features in real data are often correlated or dependent.\n",
        "\n",
        "b. Despite this simplification, Naïve Bayes classifiers perform surprisingly well in many practical applications, especially in text classification, spam filtering, and sentiment analysis.\n",
        "\n",
        "c. The algorithm applies Bayes' Theorem and models the likelihood by multiplying the conditional probabilities of each feature independently.\n",
        "\n",
        "Question 5: Describe the Gaussian, Multinomial, and Bernoulli Naïve Bayes variants.When would you use each one?\n",
        "\n",
        "**Gaussian Naïve Bayes:-**\n",
        "\n",
        "a. Assumes that the features follow a Gaussian (normal) distribution.\n",
        "\n",
        "b. Suitable for continuous-valued features (e.g., height, weight, temperature).\n",
        "\n",
        "c. The model uses the mean and variance of each feature per class to calculate probabilities.\n",
        "\n",
        "d. Use case: When the data features are continuous and approximately normally distributed, such as in sensor data, health metrics, or many classical classification datasets like Iris.\n",
        "\n",
        "**Multinomial Naive Bayes:-**\n",
        "\n",
        "a. Assumes that feature vectors represent discrete counts or frequencies (e.g., word counts in documents).\n",
        "\n",
        "b. Applies the multinomial distribution to model the likelihood of feature occurrence.\n",
        "\n",
        "c. Commonly applied in text classification tasks using bag-of-words or frequency-based features.\n",
        "\n",
        "d. Use case: When features represent counts or frequencies, such as the number of times a word appears in a document for spam filtering or sentiment analysis.\n",
        "\n",
        "**Bernoulli Naïve Bayes:-**\n",
        "\n",
        "a. Assumes binary/bool type features indicating the presence or absence of a feature (1 if feature present, 0 if absent).\n",
        "\n",
        "b. Models feature occurrence using the Bernoulli distribution.\n",
        "\n",
        "c. Suitable for binary/boolean features.\n",
        "\n",
        "d. Use case: When features are binary, such as classifying emails as spam based on presence/absence of particular keywords or detecting user clicks (yes/no) in web applications.\n",
        "\n",
        "Question 6: Write a Python program to:\n",
        "\n",
        "● Load the Iris dataset\n",
        "\n",
        "● Train an SVM Classifier with a linear kernel\n",
        "\n",
        "● Print the model's accuracy and support vectors.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qDKUTljx2sxJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMhICk0w2not",
        "outputId": "b68de6b6-259e-4548-813d-4b61f818d28c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model accuracy: 1.00\n",
            "Number of support vectors: 24\n"
          ]
        }
      ],
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train an SVM classifier with a linear kernel\n",
        "svm_clf = SVC(kernel='linear')\n",
        "svm_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = svm_clf.predict(X_test)\n",
        "\n",
        "# Print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Print number of support vectors\n",
        "print(f\"Number of support vectors: {svm_clf.support_vectors_.shape[0]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Write a Python program to:\n",
        "\n",
        "● Load the Breast Cancer dataset\n",
        "\n",
        "● Train a Gaussian Naïve Bayes model\n",
        "\n",
        "● Print its classification report including precision, recall, and F1-score.\n"
      ],
      "metadata": {
        "id": "rIkgP0ZLA9rO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load Breast Cancer dataset\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Gaussian Naive Bayes model\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Print classification report\n",
        "print(classification_report(y_test, y_pred, target_names=cancer.target_names))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kfw_GJKfBUYq",
        "outputId": "cac0ed5f-e3a6-4fa7-b695-7e5e42cc738c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "   malignant       0.93      0.90      0.92        63\n",
            "      benign       0.95      0.96      0.95       108\n",
            "\n",
            "    accuracy                           0.94       171\n",
            "   macro avg       0.94      0.93      0.94       171\n",
            "weighted avg       0.94      0.94      0.94       171\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Write a Python program to:\n",
        "\n",
        "● Train an SVM Classifier on the Wine dataset using GridSearchCV to find the best C and gamma.\n",
        "\n",
        "● Print the best hyperparameters and accuracy.\n"
      ],
      "metadata": {
        "id": "iKmLHGx7Bc1X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Wine dataset\n",
        "wine = datasets.load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define SVM classifier\n",
        "svm = SVC()\n",
        "\n",
        "# Hyperparameter grid for C and gamma (using RBF kernel)\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'gamma': ['scale', 'auto', 0.1, 1, 10],\n",
        "    'kernel': ['rbf']\n",
        "}\n",
        "\n",
        "# Setup GridSearchCV\n",
        "grid_search = GridSearchCV(svm, param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Train with grid search\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters and model\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Prediction and accuracy on test set\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Best hyperparameters: {best_params}\")\n",
        "print(f\"Test set accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yI-XY3-bB24a",
        "outputId": "559ba31f-5110-4d72-96f8-e1df95d7206e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters: {'C': 100, 'gamma': 'scale', 'kernel': 'rbf'}\n",
            "Test set accuracy: 0.78\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Write a Python program to:\n",
        "\n",
        "● Train a Naïve Bayes Classifier on a synthetic text dataset (e.g. using\n",
        "sklearn.datasets.fetch_20newsgroups).\n",
        "\n",
        "● Print the model's ROC-AUC score for its predictions."
      ],
      "metadata": {
        "id": "qMbaba6tB8UX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "# Load text dataset (subset for speed)\n",
        "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\n",
        "newsgroups = fetch_20newsgroups(subset='all', categories=categories)\n",
        "\n",
        "# Feature extraction using TF-IDF vectorizer\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X = vectorizer.fit_transform(newsgroups.data)\n",
        "y = newsgroups.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Multinomial Naive Bayes model\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities on test set\n",
        "y_prob = model.predict_proba(X_test)\n",
        "\n",
        "# Binarize the output labels for ROC-AUC computation\n",
        "y_test_binarized = label_binarize(y_test, classes=[0, 1, 2, 3])\n",
        "\n",
        "# Calculate ROC-AUC score (one-vs-rest)\n",
        "roc_auc = roc_auc_score(y_test_binarized, y_prob, average='macro', multi_class='ovr')\n",
        "\n",
        "print(f\"ROC-AUC score: {roc_auc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0g7FuOECCTMT",
        "outputId": "cddca449-6491-4fea-d86b-14532d456bc9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC score: 0.9961\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Imagine you’re working as a data scientist for a company that handles\n",
        "email communications.\n",
        "Your task is to automatically classify emails as Spam or Not Spam. The emails may\n",
        "contain:\n",
        "\n",
        "● Text with diverse vocabulary\n",
        "\n",
        "● Potential class imbalance (far more legitimate emails than spam)\n",
        "\n",
        "● Some incomplete or missing data\n",
        "\n",
        "Explain the approach you would take to:\n",
        "\n",
        "● Preprocess the data (e.g. text vectorization, handling missing data)\n",
        "\n",
        "● Choose and justify an appropriate model (SVM vs. Naïve Bayes)\n",
        "\n",
        "● Address class imbalance\n",
        "\n",
        "● Evaluate the performance of your solution with suitable metrics\n",
        "And explain the business impact of your solution.\n",
        "\n",
        "Data Preprocessing\n",
        "\n",
        "a. Text Vectorization: Use techniques like TF-IDF vectorization or word embeddings to convert email text into numeric feature vectors capturing important words and their relevancy.\n",
        "\n",
        "b. Handling Missing Data: Since emails may be incomplete, handle missing or empty fields by imputing with placeholders or ignoring them during vectorization. Also clean text by removing stopwords, punctuation, and normalizing text.\n",
        "\n",
        "c. Feature Engineering: Extract additional features like presence of links, sender reputation, punctuation patterns, etc., if available.\n",
        "\n",
        "Model Choice: SVM vs Naïve Bayes\n",
        "\n",
        "a. Naïve Bayes: Efficient for high-dimensional sparse data such as text (bag-of-words), and works well with smaller datasets. Assumes feature independence which may not always hold but handles diversity in vocabulary.\n",
        "\n",
        "b. SVM: Often more powerful with complex boundaries and able to handle overlapping classes better, especially with kernel trick. However, SVMs may take longer to train and are sensitive to tuning especially in imbalanced settings.\n",
        "\n",
        "c. Justification: Start with Multinomial Naïve Bayes for text due to efficiency and reasonable performance. Consider SVM with a linear or RBF kernel as an alternative or ensemble method if performance needs improvement.\n",
        "\n",
        "Addressing Class Imbalance\n",
        "\n",
        "a. Use techniques like:\n",
        "\n",
        "Resampling: Oversample minority class (spam) or undersample majority class (non-spam).\n",
        "\n",
        "Class Weights: Assign higher misclassification cost to minority class in SVM or Naïve Bayes.\n",
        "\n",
        "Synthetic Data: Generate synthetic spam samples using techniques like SMOTE.\n",
        "\n",
        "b. These methods help prevent the model from being biased towards the majority legitimate emails.\n",
        "\n",
        "Evaluation Metrics\n",
        "\n",
        "a. Accuracy alone is misleading in imbalanced scenarios.\n",
        "\n",
        "b. Use metrics capturing class-wise performance:\n",
        "\n",
        "c. Precision: Proportion of detected spam that is actually spam (important to minimize false alarms).\n",
        "\n",
        "d. Recall: How many actual spam emails were detected (important to catch as many spam as possible).\n",
        "\n",
        "e. F1-score: Harmonic mean of precision and recall.\n",
        "\n",
        "f. ROC-AUC: Overall capability of distinguishing spam from non-spam across thresholds.\n",
        "\n",
        "g. Also monitor confusion matrix to understand types of errors.\n",
        "\n",
        "Business Impact\n",
        "\n",
        "a. Accurate spam classification reduces user exposure to unwanted emails, improving user experience and productivity.\n",
        "\n",
        "b. Minimizing false positives avoids legitimate emails being flagged as spam, preserving customer trust.\n",
        "\n",
        "c. Automated spam filtering saves manual moderation costs and enables scalable email services.\n",
        "\n",
        "d. Enhances security by blocking phishing and malicious emails, reducing company risks.\n",
        "\n",
        "e. Overall, robust spam classification directly supports customer satisfaction, cost savings, and security compliance."
      ],
      "metadata": {
        "id": "ZgJbb-vOCfVu"
      }
    }
  ]
}